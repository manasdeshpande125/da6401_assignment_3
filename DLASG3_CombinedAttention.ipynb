{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO921ilLW+U/i9eS1A7A5Ui",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manasdeshpande125/da6401_assignment_3/blob/main/DLASG3_CombinedAttention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkfCdgBWIw8V"
      },
      "outputs": [],
      "source": [
        "import torch, os, random, numpy as np, pandas as pd\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "SEED = 42\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request, tarfile, pathlib, shutil\n",
        "\n",
        "URL = \"https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\"\n",
        "TAR = \"dakshina.tar\"\n",
        "if not pathlib.Path(TAR).exists():\n",
        "    urllib.request.urlretrieve(URL, TAR)\n",
        "    print(\"Downloaded.\")\n",
        "\n",
        "with tarfile.open(TAR) as t:\n",
        "    members = [m for m in t.getmembers() if m.name.startswith(\"dakshina_dataset_v1.0/mr/lexicons/\")]\n",
        "    t.extractall(members=members)\n",
        "DATA_ROOT = pathlib.Path(\"dakshina_dataset_v1.0/mr/lexicons\")\n",
        "print(\"Files:\", os.listdir(DATA_ROOT))"
      ],
      "metadata": {
        "id": "-50wpaTLIyPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb, yaml, json\n",
        "wandb.login(key=\"41a2853ea088e37bd0d456e78102e82edb455afc\")"
      ],
      "metadata": {
        "id": "FANaQvpLI0C6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightning"
      ],
      "metadata": {
        "id": "myhHQHDOI2UN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import lightning as L\n",
        "from lightning.pytorch.callbacks import ModelCheckpoint\n",
        "from lightning.pytorch.loggers import WandbLogger\n",
        "import wandb\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "from dataclasses import dataclass\n",
        "import torch.nn.functional as F\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    vocab_size: int\n",
        "    embedding_dim: int\n",
        "    hidden_dim: int\n",
        "    cell_type: str = 'LSTM'\n",
        "    num_encoder_layers: int = 1\n",
        "    num_decoder_layers: int = 1\n",
        "    dropout_rate: float = 0.0\n",
        "    learning_rate: float = 0.001\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, encoder_inputs: np.ndarray, decoder_inputs: np.ndarray, target_outputs: np.ndarray):\n",
        "        self.encoder_inputs = torch.from_numpy(encoder_inputs).long()\n",
        "        self.decoder_inputs = torch.from_numpy(decoder_inputs).long()\n",
        "        self.target_outputs = torch.from_numpy(target_outputs.squeeze(-1)).long()\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.encoder_inputs)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "        return {\n",
        "            'encoder_input': self.encoder_inputs[idx],\n",
        "            'decoder_input': self.decoder_inputs[idx],\n",
        "            'target': self.target_outputs[idx]\n",
        "        }\n",
        "\n",
        "class CustomAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim: int):\n",
        "        super().__init__()\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, query: torch.Tensor, keys: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        # query: (batch_size, 1, hidden_dim)\n",
        "        # keys: (batch_size, seq_len, hidden_dim)\n",
        "        query = query.unsqueeze(1)\n",
        "        energy = self.attention(torch.cat([query.expand(-1, keys.size(1), -1), keys], dim=2))\n",
        "        attention_weights = F.softmax(energy, dim=1)\n",
        "        context = torch.bmm(attention_weights.transpose(1, 2), keys)\n",
        "        return context, attention_weights\n",
        "\n",
        "class CustomSeq2Seq(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(config.vocab_size, config.embedding_dim, padding_idx=0)\n",
        "\n",
        "        if config.cell_type == 'LSTM':\n",
        "            self.encoder = nn.LSTM(\n",
        "                config.embedding_dim,\n",
        "                config.hidden_dim,\n",
        "                num_layers=config.num_encoder_layers,\n",
        "                dropout=config.dropout_rate if config.num_encoder_layers > 1 else 0,\n",
        "                batch_first=True,\n",
        "                bidirectional=True\n",
        "            )\n",
        "            self.decoder = nn.LSTM(\n",
        "                config.embedding_dim + config.hidden_dim * 2,\n",
        "                config.hidden_dim,\n",
        "                num_layers=config.num_decoder_layers,\n",
        "                dropout=config.dropout_rate if config.num_decoder_layers > 1 else 0,\n",
        "                batch_first=True\n",
        "            )\n",
        "        else:\n",
        "            self.encoder = nn.GRU(\n",
        "                config.embedding_dim,\n",
        "                config.hidden_dim,\n",
        "                num_layers=config.num_encoder_layers,\n",
        "                dropout=config.dropout_rate if config.num_encoder_layers > 1 else 0,\n",
        "                batch_first=True,\n",
        "                bidirectional=True\n",
        "            )\n",
        "            self.decoder = nn.GRU(\n",
        "                config.embedding_dim + config.hidden_dim * 2,\n",
        "                config.hidden_dim,\n",
        "                num_layers=config.num_decoder_layers,\n",
        "                dropout=config.dropout_rate if config.num_decoder_layers > 1 else 0,\n",
        "                batch_first=True\n",
        "            )\n",
        "\n",
        "        self.attention = CustomAttention(config.hidden_dim * 2)\n",
        "        self.output_layer = nn.Sequential(\n",
        "            nn.Linear(config.hidden_dim * 2, config.hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config.dropout_rate),\n",
        "            nn.Linear(config.hidden_dim, config.vocab_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, encoder_input: torch.Tensor, decoder_input: torch.Tensor) -> torch.Tensor:\n",
        "        # Encoder\n",
        "        enc_emb = self.embedding(encoder_input)\n",
        "        enc_output, enc_hidden = self.encoder(enc_emb)\n",
        "\n",
        "        # Decoder\n",
        "        dec_emb = self.embedding(decoder_input)\n",
        "\n",
        "        # Attention\n",
        "        context, _ = self.attention(enc_hidden[0][-1].unsqueeze(1), enc_output)\n",
        "        dec_input = torch.cat([dec_emb, context.expand(-1, dec_emb.size(1), -1)], dim=2)\n",
        "\n",
        "        # Decode\n",
        "        dec_output, _ = self.decoder(dec_input)\n",
        "        output = self.output_layer(dec_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "class CustomModel(L.LightningModule):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(config.__dict__)\n",
        "        self.model = CustomSeq2Seq(config)\n",
        "        self.criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "    def forward(self, encoder_input: torch.Tensor, decoder_input: torch.Tensor) -> torch.Tensor:\n",
        "        return self.model(encoder_input, decoder_input)\n",
        "\n",
        "    def training_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
        "        output = self(batch['encoder_input'], batch['decoder_input'])\n",
        "        loss = self.criterion(output.view(-1, output.size(-1)), batch['target'].view(-1))\n",
        "        self.log('train_loss', loss, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
        "        output = self(batch['encoder_input'], batch['decoder_input'])\n",
        "        loss = self.criterion(output.view(-1, output.size(-1)), batch['target'].view(-1))\n",
        "\n",
        "        pred = output.argmax(dim=-1)\n",
        "        correct = (pred == batch['target']).float().mean()\n",
        "\n",
        "        self.log('val_loss', loss, prog_bar=True)\n",
        "        self.log('val_accuracy', correct, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self) -> torch.optim.Optimizer:\n",
        "        return optim.AdamW(self.parameters(), lr=self.hparams.learning_rate)\n",
        "\n",
        "def train_custom_model(train_data: Dict[str, np.ndarray], val_data: Dict[str, np.ndarray], config: ModelConfig) -> CustomModel:\n",
        "    train_dataset = CustomDataset(\n",
        "        train_data['encoder_input'],\n",
        "        train_data['decoder_input'],\n",
        "        train_data['target']\n",
        "    )\n",
        "    val_dataset = CustomDataset(\n",
        "        val_data['encoder_input'],\n",
        "        val_data['decoder_input'],\n",
        "        val_data['target']\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=4)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, num_workers=4)\n",
        "\n",
        "    model = CustomModel(config)\n",
        "\n",
        "    wandb_logger = WandbLogger(project=\"DL_A3\", name=f\"custom_{config.cell_type}\")\n",
        "\n",
        "    trainer = L.Trainer(\n",
        "        max_epochs=config.epochs,\n",
        "        logger=wandb_logger,\n",
        "        callbacks=[\n",
        "            ModelCheckpoint(\n",
        "                dirpath='checkpoints',\n",
        "                filename='custom-model-{epoch:02d}-{val_accuracy:.2f}',\n",
        "                monitor='val_accuracy',\n",
        "                mode='max'\n",
        "            )\n",
        "        ],\n",
        "        accelerator='auto',\n",
        "        devices='auto'\n",
        "    )\n",
        "\n",
        "    trainer.fit(model, train_loader, val_loader)\n",
        "\n",
        "    return model\n",
        "\n",
        "def decode_custom_sequence(model: CustomModel, input_seq: np.ndarray, target_tokenizer: any, max_len: int = 30) -> str:\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        input_tensor = torch.from_numpy(input_seq).long().unsqueeze(0)\n",
        "        target_seq = torch.zeros((1, 1), dtype=torch.long)\n",
        "        target_seq[0, 0] = target_tokenizer.word_index['\\t']\n",
        "\n",
        "        decoded = []\n",
        "        for _ in range(max_len):\n",
        "            output = model(input_tensor, target_seq)\n",
        "            sampled_token = output[0, -1, :].argmax().item()\n",
        "            decoded.append(sampled_token)\n",
        "\n",
        "            if sampled_token == target_tokenizer.word_index['\\n']:\n",
        "                break\n",
        "\n",
        "            target_seq = torch.cat([target_seq, torch.LongTensor([[sampled_token]])], dim=1)\n",
        "\n",
        "        return ''.join([target_tokenizer.index_word.get(idx, '') for idx in decoded])"
      ],
      "metadata": {
        "id": "IaSjUSXAI4V4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, LSTM, GRU, Bidirectional, Layer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import torch\n",
        "import wandb\n",
        "from lightning import Trainer\n",
        "from lightning.pytorch.loggers import WandbLogger\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "from dataclasses import dataclass\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Enable eager execution\n",
        "tf.config.run_functions_eagerly(True)\n",
        "\n",
        "# from pytorch_implementation import (\n",
        "#     CustomDataset,\n",
        "#     CustomModel,\n",
        "#     ModelConfig,\n",
        "#     train_custom_model,\n",
        "#     decode_custom_sequence\n",
        "# )\n",
        "\n",
        "@dataclass\n",
        "class TFModelConfig:\n",
        "    vocab_size: int\n",
        "    embedding_dim: int\n",
        "    hidden_dim: int\n",
        "    cell_type: str = 'LSTM'\n",
        "    num_encoder_layers: int = 1\n",
        "    num_decoder_layers: int = 1\n",
        "    dropout_rate: float = 0.0\n",
        "    learning_rate: float = 0.001\n",
        "    batch_size: int = 64\n",
        "\n",
        "class BahdanauAttention(Layer):\n",
        "    \"\"\"TensorFlow implementation of Bahdanau attention\"\"\"\n",
        "    def __init__(self, units):\n",
        "        super().__init__()\n",
        "        self.W1 = Dense(units)\n",
        "        self.W2 = Dense(units)\n",
        "        self.V = Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        # query: (batch_size, dec_len, hidden)\n",
        "        # values: (batch_size, enc_len, hidden)\n",
        "        query_with_time_axis = tf.expand_dims(query, 2)  # (batch_size, dec_len, 1, hidden)\n",
        "        values_with_time_axis = tf.expand_dims(values, 1)  # (batch_size, 1, enc_len, hidden)\n",
        "\n",
        "        score = self.V(tf.nn.tanh(self.W1(values_with_time_axis) + self.W2(query_with_time_axis)))\n",
        "        attention_weights = tf.nn.softmax(score, axis=2)\n",
        "        context_vector = tf.reduce_sum(attention_weights * values_with_time_axis, axis=2)\n",
        "\n",
        "        return context_vector, tf.squeeze(attention_weights, -1)\n",
        "\n",
        "def build_tf_model(config: TFModelConfig) -> Model:\n",
        "    \"\"\"Build TensorFlow model with attention\"\"\"\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(None,))\n",
        "    encoder_emb = Embedding(config.vocab_size, config.embedding_dim, mask_zero=True)(encoder_inputs)\n",
        "\n",
        "    if config.cell_type == 'LSTM':\n",
        "        encoder = LSTM(\n",
        "            config.hidden_dim,\n",
        "            return_sequences=True,\n",
        "            return_state=True,\n",
        "            dropout=config.dropout_rate\n",
        "        )\n",
        "        encoder_outputs, state_h, state_c = encoder(encoder_emb)\n",
        "        encoder_states = [state_h, state_c]\n",
        "    else:\n",
        "        encoder = GRU(\n",
        "            config.hidden_dim,\n",
        "            return_sequences=True,\n",
        "            return_state=True,\n",
        "            dropout=config.dropout_rate\n",
        "        )\n",
        "        encoder_outputs, state_h = encoder(encoder_emb)\n",
        "        encoder_states = [state_h]\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = Input(shape=(None,))\n",
        "    decoder_emb = Embedding(config.vocab_size, config.embedding_dim, mask_zero=True)(decoder_inputs)\n",
        "\n",
        "    if config.cell_type == 'LSTM':\n",
        "        decoder = LSTM(\n",
        "            config.hidden_dim,\n",
        "            return_sequences=True,\n",
        "            return_state=True,\n",
        "            dropout=config.dropout_rate\n",
        "        )\n",
        "        decoder_outputs, _, _ = decoder(decoder_emb, initial_state=encoder_states)\n",
        "    else:\n",
        "        decoder = GRU(\n",
        "            config.hidden_dim,\n",
        "            return_sequences=True,\n",
        "            return_state=True,\n",
        "            dropout=config.dropout_rate\n",
        "        )\n",
        "        decoder_outputs, _ = decoder(decoder_emb, initial_state=encoder_states)\n",
        "\n",
        "    # Attention\n",
        "    attention_layer = BahdanauAttention(config.hidden_dim)\n",
        "    context_vector, attention_weights = attention_layer(decoder_outputs, encoder_outputs)\n",
        "\n",
        "    # Concatenate context vector and decoder output\n",
        "    concat = tf.keras.layers.Concatenate()([decoder_outputs, context_vector])\n",
        "\n",
        "    # Output layer\n",
        "    outputs = Dense(config.vocab_size, activation='softmax')(concat)\n",
        "\n",
        "    # Create model\n",
        "    model = Model([encoder_inputs, decoder_inputs], outputs)\n",
        "\n",
        "    # Store attention weights for visualization\n",
        "    model.attention_weights = attention_weights\n",
        "\n",
        "    return model\n",
        "\n",
        "def load_and_preprocess_data(data_dir: str) -> Tuple[Dict[str, Dict[str, np.ndarray]], Tokenizer, Tokenizer, int, int]:\n",
        "    \"\"\"Load and preprocess data for both frameworks\"\"\"\n",
        "    def load_tsv(path: str) -> List[List[str]]:\n",
        "        with open(path, encoding='utf-8') as f:\n",
        "            lines = f.read().strip().split('\\n')\n",
        "        return [line.split('\\t') for line in lines if '\\t' in line]\n",
        "\n",
        "    # Load data\n",
        "    train_pairs = load_tsv(os.path.join(data_dir, \"mr.translit.sampled.train.tsv\"))\n",
        "    val_pairs = load_tsv(os.path.join(data_dir, \"mr.translit.sampled.dev.tsv\"))\n",
        "    test_pairs = load_tsv(os.path.join(data_dir, \"mr.translit.sampled.test.tsv\"))\n",
        "\n",
        "    # Tokenize\n",
        "    def tokenize_pairs(pairs: List[List[str]]) -> Tuple[List[str], List[str], List[str]]:\n",
        "        latin_texts = [x[1] for x in pairs]\n",
        "        marathi_texts = [x[0] for x in pairs]\n",
        "        marathi_texts_in = ['\\t' + t for t in marathi_texts]\n",
        "        marathi_texts_out = [t + '\\n' for t in marathi_texts]\n",
        "        return latin_texts, marathi_texts_in, marathi_texts_out\n",
        "\n",
        "    train_lat, train_mr_in, train_mr_out = tokenize_pairs(train_pairs)\n",
        "    val_lat, val_mr_in, val_mr_out = tokenize_pairs(val_pairs)\n",
        "    test_lat, test_mr_in, test_mr_out = tokenize_pairs(test_pairs)\n",
        "\n",
        "    # Create tokenizers\n",
        "    input_tokenizer = Tokenizer(char_level=True, lower=False)\n",
        "    input_tokenizer.fit_on_texts(train_lat + val_lat)\n",
        "\n",
        "    target_tokenizer = Tokenizer(char_level=True, lower=False)\n",
        "    target_tokenizer.fit_on_texts(train_mr_in + train_mr_out)\n",
        "\n",
        "    # Get vocabulary sizes\n",
        "    vocab_size_input = len(input_tokenizer.word_index) + 1\n",
        "    vocab_size_target = len(target_tokenizer.word_index) + 1\n",
        "\n",
        "    # Get max lengths\n",
        "    maxlen_input = max(map(len, train_lat))\n",
        "    maxlen_target = max(map(len, train_mr_out))\n",
        "\n",
        "    # Encode and pad sequences\n",
        "    def encode_and_pad(texts: List[str], tokenizer: Tokenizer, maxlen: Optional[int] = None) -> np.ndarray:\n",
        "        return pad_sequences(tokenizer.texts_to_sequences(texts), padding='post', maxlen=maxlen)\n",
        "\n",
        "    # Prepare data for both frameworks\n",
        "    data = {\n",
        "        'train': {\n",
        "            'encoder_input': encode_and_pad(train_lat, input_tokenizer, maxlen_input),\n",
        "            'decoder_input': encode_and_pad(train_mr_in, target_tokenizer, maxlen_target),\n",
        "            'target': np.expand_dims(encode_and_pad(train_mr_out, target_tokenizer, maxlen_target), -1)\n",
        "        },\n",
        "        'val': {\n",
        "            'encoder_input': encode_and_pad(val_lat, input_tokenizer, maxlen_input),\n",
        "            'decoder_input': encode_and_pad(val_mr_in, target_tokenizer, maxlen_target),\n",
        "            'target': np.expand_dims(encode_and_pad(val_mr_out, target_tokenizer, maxlen_target), -1)\n",
        "        },\n",
        "        'test': {\n",
        "            'encoder_input': encode_and_pad(test_lat, input_tokenizer, maxlen_input),\n",
        "            'decoder_input': encode_and_pad(test_mr_in, target_tokenizer, maxlen_target),\n",
        "            'target': np.expand_dims(encode_and_pad(test_mr_out, target_tokenizer, maxlen_target), -1)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return data, input_tokenizer, target_tokenizer, vocab_size_input, vocab_size_target\n",
        "\n",
        "def train_models(data: Dict[str, Dict[str, np.ndarray]], vocab_size_input: int, vocab_size_target: int) -> Tuple[Model, CustomModel]:\n",
        "    \"\"\"Train both TensorFlow and PyTorch models\"\"\"\n",
        "\n",
        "    # Common configuration\n",
        "    tf_config = TFModelConfig(\n",
        "        vocab_size=vocab_size_target,\n",
        "        embedding_dim=256,\n",
        "        hidden_dim=256,\n",
        "        cell_type='LSTM',\n",
        "        num_encoder_layers=2,\n",
        "        num_decoder_layers=2,\n",
        "        dropout_rate=0.2,\n",
        "        learning_rate=0.001,\n",
        "        batch_size=64\n",
        "    )\n",
        "\n",
        "    pt_config = ModelConfig(\n",
        "        vocab_size=vocab_size_target,\n",
        "        embedding_dim=256,\n",
        "        hidden_dim=256,\n",
        "        cell_type='LSTM',\n",
        "        num_encoder_layers=2,\n",
        "        num_decoder_layers=2,\n",
        "        dropout_rate=0.2,\n",
        "        learning_rate=0.001,\n",
        "        batch_size=64\n",
        "    )\n",
        "\n",
        "    # Initialize wandb\n",
        "    wandb.init(project=\"DA6401_Assignment_3\", name=\"combined_implementation\")\n",
        "\n",
        "    # Train TensorFlow model\n",
        "    tf_model = build_tf_model(tf_config)\n",
        "    tf_model.compile(\n",
        "        optimizer=Adam(learning_rate=tf_config.learning_rate),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Create a custom callback that only logs metrics\n",
        "    class CustomWandbCallback(tf.keras.callbacks.Callback):\n",
        "        def on_epoch_end(self, epoch, logs=None):\n",
        "            if logs:\n",
        "                wandb.log(logs)\n",
        "\n",
        "    tf_model.fit(\n",
        "        [data['train']['encoder_input'], data['train']['decoder_input']],\n",
        "        data['train']['target'],\n",
        "        validation_data=([data['val']['encoder_input'], data['val']['decoder_input']], data['val']['target']),\n",
        "        batch_size=tf_config.batch_size,\n",
        "        epochs=10,\n",
        "        callbacks=[CustomWandbCallback()]\n",
        "    )\n",
        "\n",
        "    # Train PyTorch model\n",
        "    pt_model = train_custom_model(data['train'], data['val'], pt_config)\n",
        "\n",
        "    return tf_model, pt_model\n",
        "\n",
        "def evaluate_models(tf_model: Model, pt_model: CustomModel, test_data: Dict[str, np.ndarray], target_tokenizer: Tokenizer) -> Tuple[float, float]:\n",
        "    \"\"\"Evaluate both models on test data\"\"\"\n",
        "\n",
        "    # Evaluate TensorFlow model\n",
        "    tf_loss, tf_acc = tf_model.evaluate(\n",
        "        [test_data['encoder_input'], test_data['decoder_input']],\n",
        "        test_data['target']\n",
        "    )\n",
        "    print(f\"TensorFlow Test Accuracy: {tf_acc:.4f}\")\n",
        "\n",
        "    # Evaluate PyTorch model\n",
        "    pt_model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_dataset = CustomDataset(\n",
        "            test_data['encoder_input'],\n",
        "            test_data['decoder_input'],\n",
        "            test_data['target']\n",
        "        )\n",
        "        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, num_workers=4)\n",
        "\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for batch in test_loader:\n",
        "            output = pt_model(batch['encoder_input'], batch['decoder_input'])\n",
        "            pred = output.argmax(dim=-1)\n",
        "            correct += (pred == batch['target']).float().sum().item()\n",
        "            total += batch['target'].numel()\n",
        "\n",
        "        pt_acc = correct / total\n",
        "        print(f\"PyTorch Test Accuracy: {pt_acc:.4f}\")\n",
        "\n",
        "    return tf_acc, pt_acc\n",
        "\n",
        "def plot_attention_heatmap(input_text: str, output_text: str, attention_weights: np.ndarray, idx: int = 1):\n",
        "    \"\"\"Plot attention heatmap for a single sample\"\"\"\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    ax = sns.heatmap(\n",
        "        attention_weights,\n",
        "        xticklabels=list(input_text),\n",
        "        yticklabels=list(output_text),\n",
        "        cmap='magma',\n",
        "        cbar=False,\n",
        "        linewidths=0.5,\n",
        "        annot=False\n",
        "    )\n",
        "    plt.xlabel(\"Input (Latin)\")\n",
        "    plt.ylabel(\"Output (Hindi)\")\n",
        "    plt.title(f\"Sample {idx} Attention\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_9_grid(test_lat: List[str], test_encoder_input: np.ndarray, model: CustomModel, target_tokenizer: Tokenizer):\n",
        "    \"\"\"Plot 9 attention heatmaps in a grid\"\"\"\n",
        "    plt.figure(figsize=(18, 15))\n",
        "    for i in range(9):\n",
        "        input_text = test_lat[i]\n",
        "        input_seq = test_encoder_input[i:i+1]\n",
        "        output_text, attn_weights = decode_custom_sequence(model, input_seq, target_tokenizer)\n",
        "        attn_matrix = np.stack(attn_weights)\n",
        "\n",
        "        plt.subplot(3, 3, i+1)\n",
        "        sns.heatmap(attn_matrix, xticklabels=list(input_text), yticklabels=list(output_text), cmap='coolwarm', cbar=False)\n",
        "        plt.title(f\"Input: {input_text}\")\n",
        "        plt.xlabel(\"Latin chars\")\n",
        "        plt.ylabel(\"Hindi chars\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def save_attention_heatmaps(test_lat: List[str], test_encoder_input: np.ndarray, model: CustomModel, target_tokenizer: Tokenizer, num_samples: int = 10):\n",
        "    \"\"\"Save attention heatmaps for multiple samples\"\"\"\n",
        "    os.makedirs(\"attention_heatmaps\", exist_ok=True)\n",
        "    for i in range(num_samples):\n",
        "        input_text = test_lat[i]\n",
        "        input_seq = test_encoder_input[i:i+1]\n",
        "        output_text, attn_weights = decode_custom_sequence(model, input_seq, target_tokenizer)\n",
        "        attn_matrix = np.stack(attn_weights)\n",
        "\n",
        "        plt.figure(figsize=(6, 5))\n",
        "        sns.heatmap(attn_matrix, xticklabels=list(input_text), yticklabels=list(output_text), cmap='plasma')\n",
        "        plt.title(f\"Sample {i+1}\")\n",
        "        plt.xlabel(\"Input (Latin)\")\n",
        "        plt.ylabel(\"Output (Hindi)\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"attention_heatmaps/sample_{i+1}.png\")\n",
        "        plt.close()\n",
        "\n",
        "def create_wandb_table(test_lat: List[str], decoded_preds: List[str], decoded_refs: List[str], num_samples: int = 10):\n",
        "    \"\"\"Create and log a wandb table with predictions\"\"\"\n",
        "    wandb.init(project=\"seq2seq_sweep\", name=\"prediction_samples_colored_table\")\n",
        "\n",
        "    table = wandb.Table(columns=[\"Input Word\", \"Predicted Word\", \"Target Word\"])\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        input_word = test_lat[i]\n",
        "        pred_word = decoded_preds[i]\n",
        "        target_word = decoded_refs[i]\n",
        "\n",
        "        color = \"#00FF00\" if pred_word == target_word else \"#FF0000\"\n",
        "        colored_pred = f'<span style=\"color: {color}\">{pred_word}</span>'\n",
        "\n",
        "        table.add_data(input_word, colored_pred, target_word)\n",
        "\n",
        "    wandb.log({\"Prediction Samples Colored Table\": table})\n",
        "    wandb.finish()\n",
        "\n",
        "def sweep_train_tf(data: Dict[str, Dict[str, np.ndarray]]):\n",
        "    \"\"\"Train TensorFlow model with wandb sweep\"\"\"\n",
        "    with wandb.init(project=\"DA6401_Assignment_3\", entity=\"cs24m024-iit-madras\"):\n",
        "        config = wandb.config\n",
        "        wandb.run.name = f\"tf_{config.cell_type}_emb{config.embedding_dim}_hid{config.hidden_dim}_enc{config.num_encoder_layers}_dec{config.num_decoder_layers}_drop{int(config.dropout_rate*100)}\"\n",
        "\n",
        "        tf_config = TFModelConfig(\n",
        "            vocab_size=config.vocab_size,\n",
        "            embedding_dim=config.embedding_dim,\n",
        "            hidden_dim=config.hidden_dim,\n",
        "            cell_type=config.cell_type,\n",
        "            num_encoder_layers=config.num_encoder_layers,\n",
        "            num_decoder_layers=config.num_decoder_layers,\n",
        "            dropout_rate=config.dropout_rate,\n",
        "            learning_rate=config.learning_rate,\n",
        "            batch_size=config.batch_size\n",
        "        )\n",
        "\n",
        "        model = build_tf_model(tf_config)\n",
        "        model.compile(\n",
        "            optimizer=Adam(learning_rate=config.learning_rate),\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        # Create a custom callback that only logs metrics\n",
        "        class CustomWandbCallback(tf.keras.callbacks.Callback):\n",
        "            def on_epoch_end(self, epoch, logs=None):\n",
        "                if logs:\n",
        "                    wandb.log(logs)\n",
        "\n",
        "        model.fit(\n",
        "            [data['train']['encoder_input'], data['train']['decoder_input']],\n",
        "            data['train']['target'],\n",
        "            validation_data=([data['val']['encoder_input'], data['val']['decoder_input']], data['val']['target']),\n",
        "            batch_size=config.batch_size,\n",
        "            epochs=config.epochs,\n",
        "            callbacks=[CustomWandbCallback()]\n",
        "        )\n",
        "\n",
        "global data\n",
        "\n",
        "def sweep_train_pt(config=None):\n",
        "    global data\n",
        "    \"\"\"Train PyTorch model with wandb sweep\"\"\"\n",
        "    with wandb.init(config=config, project=\"DA6401_Assignment_3\", entity=\"cs24m024-iit-madras\"):\n",
        "        config = wandb.config\n",
        "        wandb.run.name = f\"pt_{config.cell_type}_emb{config.embedding_dim}_hid{config.hidden_dim}_enc{config.num_encoder_layers}_dec{config.num_decoder_layers}_drop{int(config.dropout_rate*100)}\"\n",
        "\n",
        "        pt_config = ModelConfig(\n",
        "            vocab_size=config.vocab_size,\n",
        "            embedding_dim=config.embedding_dim,\n",
        "            hidden_dim=config.hidden_dim,\n",
        "            cell_type=config.cell_type,\n",
        "            num_encoder_layers=config.num_encoder_layers,\n",
        "            num_decoder_layers=config.num_decoder_layers,\n",
        "            dropout_rate=config.dropout_rate,\n",
        "            learning_rate=config.learning_rate,\n",
        "            batch_size=config.batch_size\n",
        "        )\n",
        "\n",
        "        model = train_custom_model(data['train'], data['val'], pt_config)\n",
        "        return model\n",
        "\n",
        "def run_sweeps(data_arg: Dict[str, Dict[str, np.ndarray]], vocab_size: int):\n",
        "    global data\n",
        "    data = data_arg\n",
        "    # TensorFlow sweep config\n",
        "    tf_sweep_config = {\n",
        "        'method': 'bayes',\n",
        "        'metric': {'name': 'val_accuracy', 'goal': 'maximize'},\n",
        "        'parameters': {\n",
        "            'vocab_size': {'value': vocab_size},\n",
        "            'embedding_dim': {'values': [16, 32, 64, 256]},\n",
        "            'hidden_dim': {'values': [16, 32, 64, 256]},\n",
        "            'cell_type': {'values': ['RNN', 'LSTM']},\n",
        "            'num_encoder_layers': {'values': [1, 2]},\n",
        "            'num_decoder_layers': {'values': [1, 2]},\n",
        "            'dropout_rate': {'values': [0.2, 0.3]},\n",
        "            'batch_size': {'values': [32, 64]},\n",
        "            'epochs': {'value': 1},\n",
        "            'learning_rate': {'value': 0.001}\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # PyTorch sweep config\n",
        "    pt_sweep_config = {\n",
        "        'method': 'bayes',\n",
        "        'metric': {'name': 'val_accuracy', 'goal': 'maximize'},\n",
        "        'parameters': {\n",
        "            'vocab_size': {'value': vocab_size},\n",
        "            'embedding_dim': {'values': [16, 32, 64, 256]},\n",
        "            'hidden_dim': {'values': [16, 32, 64, 256]},\n",
        "            'cell_type': {'values': ['RNN', 'LSTM']},\n",
        "            'num_encoder_layers': {'values': [1, 2]},\n",
        "            'num_decoder_layers': {'values': [1, 2]},\n",
        "            'dropout_rate': {'values': [0.2, 0.3]},\n",
        "            'batch_size': {'values': [64]},\n",
        "            'epochs': {'value': 1},\n",
        "            'learning_rate': {'value': 0.001}\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Run TensorFlow sweep\n",
        "    def sweep_train_tf_wrapper():\n",
        "        sweep_train_tf(data)\n",
        "    tf_sweep_id = wandb.sweep(tf_sweep_config, project=\"DA6401_Assignment_3\", entity=\"cs24m024-iit-madras\")\n",
        "    wandb.agent(tf_sweep_id, function=sweep_train_tf_wrapper, count=1)\n",
        "\n",
        "    # Run PyTorch sweep\n",
        "    pt_sweep_id = wandb.sweep(pt_sweep_config, project=\"DA6401_Assignment_3\", entity=\"cs24m024-iit-madras\")\n",
        "    wandb.agent(pt_sweep_id, function=sweep_train_pt, count=1)\n",
        "\n",
        "def main():\n",
        "    # Load and preprocess data\n",
        "    data_dir = \"/kaggle/working/dakshina_dataset_v1.0/mr/lexicons\"\n",
        "    data, input_tokenizer, target_tokenizer, vocab_size_input, vocab_size_target = load_and_preprocess_data(data_dir)\n",
        "\n",
        "    # Run sweeps\n",
        "    run_sweeps(data, vocab_size_target)\n",
        "\n",
        "    # Train models with best configurations\n",
        "    tf_model, pt_model = train_models(data, vocab_size_input, vocab_size_target)\n",
        "\n",
        "    # Evaluate models\n",
        "    tf_acc, pt_acc = evaluate_models(tf_model, pt_model, data['test'], target_tokenizer)\n",
        "\n",
        "    # Generate predictions\n",
        "    decoded_preds = [decode_custom_sequence(pt_model, data['test']['encoder_input'][i:i+1], target_tokenizer)\n",
        "                    for i in range(len(data['test']['encoder_input']))]\n",
        "    decoded_refs = [t.replace(' </s>', '') for t in test_mr_out]\n",
        "\n",
        "    # Create wandb table\n",
        "    create_wandb_table(test_lat, decoded_preds, decoded_refs)\n",
        "\n",
        "    # Plot attention heatmaps\n",
        "    plot_9_grid(test_lat, data['test']['encoder_input'], pt_model, target_tokenizer)\n",
        "    save_attention_heatmaps(test_lat, data['test']['encoder_input'], pt_model, target_tokenizer)\n",
        "\n",
        "    # Save models\n",
        "    tf_model.save(\"best_tf_model.keras\")\n",
        "    torch.save(pt_model.state_dict(), \"best_pt_model.pt\")\n",
        "\n",
        "    # Log results\n",
        "    wandb.log({\n",
        "        \"tf_test_accuracy\": tf_acc,\n",
        "        \"pt_test_accuracy\": pt_acc\n",
        "    })\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "rzj1BGnSI78W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}